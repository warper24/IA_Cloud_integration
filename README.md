Tests fonction

Microservices pour chaque fonctionnalité

Fast API pour le model

Interface web






Triton serveur d'inferences

si triton recoit audio, la conversion se fait en fastAPI 


Flask Readiss

Le mieux c pas de video dans triton


Pas bcp d'util --> lancer sur process unique chaque tache

Bcp d'util --> readiss qui le fait pour nous 

quel serveur d'inferences sont optimals pour modèle Whisper --> VLLM

V LLM c par 30 secondes de video
TRT LLM : technologie backend dus serrveur python  Chercher TRT LLM Whisper.
Le catch peut être long

Chercher quel serveur peut convenir pour nos besoins de charges d'utilisateurs et en fonction du backend